<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>OpenAI gave me access to DALL-E 2 and here is my opinion.</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">OpenAI gave me access to DALL-E 2 and here is my opinion.</h1>
</header>
<section data-field="subtitle" class="p-summary">
So, I requested OpenAI for an access to test out its amazing image generation AI called DALL-E 2. Honestly I was inspired by DALL-E when it‚Ä¶
</section>
<section data-field="body" class="e-content">
<section name="3947" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="8d9d" id="8d9d" class="graf graf--h3 graf--leading graf--title">OpenAI gave me access to DALL-E 2 and here is my¬†opinion.</h3><figure name="6bc4" id="6bc4" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*J_zlPFzqkvEugpgsfl-suQ.png" data-width="1024" data-height="1024" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*J_zlPFzqkvEugpgsfl-suQ.png"><figcaption class="imageCaption">Image generated by DALL-E¬†2</figcaption></figure><p name="1551" id="1551" class="graf graf--p graf-after--figure">So, I requested OpenAI for an access to test out its amazing image generation AI called DALL-E 2. Honestly I was inspired by DALL-E when it first came out and thats why I wrote one of my most famous article which was called: <a href="https://medium.com/@eeman.majumder/making-an-ai-to-make-beautiful-art-using-gpt-3-clip-and-the-cc12m-dataset-144677848e96" data-href="https://medium.com/@eeman.majumder/making-an-ai-to-make-beautiful-art-using-gpt-3-clip-and-the-cc12m-dataset-144677848e96" class="markup--anchor markup--p-anchor" target="_blank">Making an AI to make beautiful Art using GPT-3, CLIP and the CC12M Dataset</a> DALL-E was the AI that Inspired me to make this AI.</p><p name="63b9" id="63b9" class="graf graf--p graf-after--p">Lets first find out how DALL-2 Works:</p><h3 name="3ed3" id="3ed3" class="graf graf--h3 graf-after--p">CLIP</h3><p name="8ad8" id="8ad8" class="graf graf--p graf-after--h3">One of the key components of the DALL-E 2 architecture is CLIP. CLIP is important to DALL-E 2 because it allows text and images to communicate with each other. This means that language can be used to teach computers how different images are related to each other. This fluency prescribes a simple way to carry out this teaching.</p><p name="9743" id="9743" class="graf graf--p graf-after--p">To best understand CLIP, it is helpful to see how it improves upon the shortcomings of previous computer vision systems. This means that until CLIP, neural methods for computer vision involved taking a large dataset of images and then hand labeling them into a set of categories. Even though today‚Äôs models are really good at this task, there is still a limit to how well they can do because they need to have categories that are already selected. For example, if you took a picture of a street and asked a system to describe it, it could tell you how many cars and signs there were, but it wouldn‚Äôt be able to give you a feel for the scene as a whole. If there aren‚Äôt enough images to produce a category, the model won‚Äôt classify it.</p><p name="8b12" id="8b12" class="graf graf--p graf-after--p">CLIP‚Äôs success comes from its ability to train models to not just identify an image‚Äôs category from a pre-defined list, but to also identify the caption of the image from a list of random captions. This allows the model to use language to more precisely understand the difference between two things, rather than having a human labeler dictate in advance whether or not these belong in the same category. CLIP is able to create a vector space representing both features of images and features of language by completing the ‚Äòpre-training task‚Äô described above. The shared vector space effectively provides models with a dictionary that translates images and text, allowing for at least a semantical understanding between the two.</p><p name="8211" id="8211" class="graf graf--p graf-after--p">CLIP is able to create a vector space representing both image and language features, allowing for training on both inputs. This shared vector space allows models to effectively translate between images and text, providing a sort of image-text dictionary. This allows for at least a semantic understanding between the two.0‚Äôs capabilities DALL-E 2.0 is capable of much more than just sharing an understanding with others. In order to speak Spanish fluently, one would need to learn not only how to translate English words into their Spanish counterparts, but also how to properly pronounce Spanish words and use correct Spanish grammar. CLIP allows us to see how textual phrases connect to images, providing us with a deeper understanding of the meaning behind the words. We need a way to generate images that accurately represent our understanding of the text. The diffusion model is our second lego building block. It allows us to see how information spreads through a population.</p><h3 name="9012" id="9012" class="graf graf--h3 graf-after--p">Diffusion Models</h3><p name="da85" id="da85" class="graf graf--p graf-after--h3">Assume you have a Rubik‚Äôs Cube that is perfectly solved, with only blocks of a single colour on each side. You now pick a side at random and twist it. Then another side, and another, and another, and so on, until the Rubik‚Äôs Cube is considered‚Äôscrambled.‚Äô How would you approach the problem? If you forget all of your twists, is there a general way to solve the cube without that information? You could simply take it one step at a time, twisting whatever face brings you closer to having all the same colour on the same side until the problem is solved. More practically, you could train a neural network to go from disorder to less disorder.</p><p name="23ee" id="23ee" class="graf graf--p graf-after--p">This is where diffusion models come into play. You start with an image, randomly scramble its pixels until you have a pure noise image, and then train a model to reduce the noise by changing its pixels step by step until you get back to the original image (or something that resembles it). This results in a model that can generate information‚Ää‚Äî‚Ääan image‚Ää‚Äî‚Ääfrom randomness! We can even get new images by showing the model new, random samples to start from.</p><p name="d5a3" id="d5a3" class="graf graf--p graf-after--p">We ‚Äòcondition‚Äô the diffusion model on the CLIP embeddings to infuse these creations with the semantic meaning of our input text. This simply means that we feed the vectors from our previously described joint CLIP space into the diffusion model, which calculates which pixels to change at each‚Äôstep‚Äô of the generation process. This enables the model to base its changes on that data.</p><p name="cb4b" id="cb4b" class="graf graf--p graf-after--p">The technical details of the math and implementation are, of course, more complicated than I have presented here. DALL-E 2 is, at its core is the optimization and refinement of these two technologies.</p><p name="d17f" id="d17f" class="graf graf--p graf-after--p">Visit there official website for more details:</p><div name="c189" id="c189" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://openai.com/dall-e-2/" data-href="https://openai.com/dall-e-2/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://openai.com/dall-e-2/"><strong class="markup--strong markup--mixtapeEmbed-strong">DALL¬∑E 2</strong><br>openai.com</a><a href="https://openai.com/dall-e-2/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="0a79a9095a902189042a24abc91f6c19" data-thumbnail-img-id="0*q3gL51ek7rZtnh0V" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*q3gL51ek7rZtnh0V);"></a></div><p name="8433" id="8433" class="graf graf--p graf-after--mixtapeEmbed">Now that you know how DALL-E 2 works lets see some of its amazing outputs:</p><figure name="4b25" id="4b25" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*XVFGGYwm_IGqKr7snNyjXQ.png" data-width="2872" data-height="1812" src="https://cdn-images-1.medium.com/max/800/1*XVFGGYwm_IGqKr7snNyjXQ.png"><figcaption class="imageCaption">A fish tank on a marble table in a black room with a sea horse in the fish¬†tank</figcaption></figure><figure name="6578" id="6578" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*e0wRrB7_el9_iH5qNe4lWA.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*e0wRrB7_el9_iH5qNe4lWA.png"></figure><figure name="dddb" id="dddb" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*cIWm9JYb7RI9gbF-sRjj1g.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*cIWm9JYb7RI9gbF-sRjj1g.png"></figure><figure name="b9a9" id="b9a9" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*HnZPT7KqD7o3Ff0jRFiRzQ.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*HnZPT7KqD7o3Ff0jRFiRzQ.png"></figure><figure name="27ae" id="27ae" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*aryWZWriQ8DtA_g-BMa2qw.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*aryWZWriQ8DtA_g-BMa2qw.png"></figure><figure name="2281" id="2281" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*tnSPlobxJxbNhQAPBUYHSw.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*tnSPlobxJxbNhQAPBUYHSw.png"></figure><p name="f8f9" id="f8f9" class="graf graf--p graf-after--figure">The results are absolutely mind numbing like how are they an AI generated image they are so photo-real ahhh this is why I love AI.</p><p name="148b" id="148b" class="graf graf--p graf-after--p">Lets see some more results:</p><figure name="4196" id="4196" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*UfGkKzgYv9Xa7GRiokZnNw.png" data-width="2872" data-height="1812" src="https://cdn-images-1.medium.com/max/800/1*UfGkKzgYv9Xa7GRiokZnNw.png"><figcaption class="imageCaption">A pink mustard bottle on a pool with children playing in a park in the background</figcaption></figure><figure name="ba52" id="ba52" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*tpgaPwcL0JFRoYcBxOwwOg.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*tpgaPwcL0JFRoYcBxOwwOg.png"></figure><figure name="0e4f" id="0e4f" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*6_YIVY4ySWIjcWWzy_7snQ.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*6_YIVY4ySWIjcWWzy_7snQ.png"></figure><figure name="8288" id="8288" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*ZHrK_PbMkHFDFEBRXixdFg.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*ZHrK_PbMkHFDFEBRXixdFg.png"></figure><figure name="f12a" id="f12a" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*sPjoE8fxMY3F9BvxLWg9iw.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*sPjoE8fxMY3F9BvxLWg9iw.png"></figure><figure name="541b" id="541b" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*Reu23SdzxWWPJZMn3RUWAw.png" data-width="1024" data-height="1024" src="https://cdn-images-1.medium.com/max/800/1*Reu23SdzxWWPJZMn3RUWAw.png"></figure><p name="0260" id="0260" class="graf graf--p graf-after--figure">Honestly no wonder they won the Turing test. These generations are absolutely flawless.</p><p name="8667" id="8667" class="graf graf--p graf-after--p">So, here is my final review on DALL-E 2. It reminded me why I love AI so much like this is basically giving your imagination to someone to make it into a reality. There is literally infinite possibilities with this thing. Honestly hats off the OpenAI and keep up the good work.</p><p name="9d0f" id="9d0f" class="graf graf--p graf-after--p"><a href="https://medium.com/@eeman.majumder" data-href="https://medium.com/@eeman.majumder" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">If you liked this article follow me on medium for more stuff. I make custom AIs for fun and write about them.</strong></a></p><p name="b097" id="b097" class="graf graf--p graf-after--p">For more of my works check out my Github:</p><div name="5698" id="5698" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/Eeman1113/Generating-Art-using-AI" data-href="https://github.com/Eeman1113/Generating-Art-using-AI" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/Eeman1113/Generating-Art-using-AI"><strong class="markup--strong markup--mixtapeEmbed-strong">GitHub - Eeman1113/Generating-Art-using-AI</strong></a><a href="https://github.com/Eeman1113/Generating-Art-using-AI" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="00a7eb566e48efaedb5d9ee92ee5c8f5" data-thumbnail-img-id="0*SAPe-OTAorRgyPMt" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*SAPe-OTAorRgyPMt);"></a></div><p name="e8df" id="e8df" class="graf graf--p graf-after--mixtapeEmbed">For my day to day AIML updates follow me on twitter:</p><figure name="5875" id="5875" class="graf graf--figure graf--iframe graf-after--p"><blockquote class="twitter-tweet"><a href="https://twitter.com/EemanMajumder/status/1535596247586811904"></a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><figcaption class="imageCaption">thanks for hiring me @ <a href="https://app.manuscripts.ai" data-href="https://app.manuscripts.ai" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">manuscripts.ai</a></figcaption></figure><p name="cfbf" id="cfbf" class="graf graf--p graf-after--figure graf--trailing">Thanks for reading üòÅ, See ya guys next week üëãüèº.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@eeman.majumder" class="p-author h-card">Eeman Majumder</a> on <a href="https://medium.com/p/9dd9cc72b6ae"><time class="dt-published" datetime="2022-06-17T16:25:19.310Z">June 17, 2022</time></a>.</p><p><a href="https://medium.com/@eeman.majumder/openai-gave-me-access-to-dall-e-2-and-here-is-my-opinion-9dd9cc72b6ae" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on August 6, 2024.</p></footer></article></body></html>