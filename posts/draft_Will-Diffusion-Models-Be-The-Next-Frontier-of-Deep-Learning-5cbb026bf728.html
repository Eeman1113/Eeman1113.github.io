<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Will Diffusion Models Be The Next Frontier of Deep Learning</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Will Diffusion Models Be The Next Frontier of Deep Learning</h1>
</header>
<section data-field="subtitle" class="p-summary">
Executive Highlights (tl;dr of the article)
</section>
<section data-field="body" class="e-content">
<section name="e5b9" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="da31" id="da31" class="graf graf--h3 graf--leading graf--title">Will Diffusion Models Be The Next Frontier of Deep Learning</h3><figure name="11a2" id="11a2" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*ihT9sKhC41AUEOvZ" data-width="1400" data-height="1867" src="https://cdn-images-1.medium.com/max/800/0*ihT9sKhC41AUEOvZ"></figure><h3 name="c474" id="c474" class="graf graf--h3 graf-after--figure">Executive Highlights (tl;dr of the article)</h3><p name="f8d2" id="f8d2" class="graf graf--p graf--empty graf-after--h3"><br></p><p name="8211" id="8211" class="graf graf--p graf-after--p">Google’s AlphaFold 3 is gaining a lot of attention for its potential to revolutionize bio-tech. One of the key innovations that led to its performance gains over previous methods was its utilization of diffusion models</p><blockquote name="cc69" id="cc69" class="graf graf--blockquote graf-after--p"><em class="markup--em markup--blockquote-em">AlphaFold 3’s capabilities come from its next-generation architecture and training that now covers all of life’s molecules. At the core of the model is an improved version of our Evoformer module — a deep learning architecture that underpinned AlphaFold 2’s incredible performance. </em><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">After processing the inputs, AlphaFold 3 assembles its predictions using a diffusion network, akin to those found in AI image generators. The diffusion process starts with a cloud of atoms, and over many steps converges on its final, most accurate molecular structure.</em></strong></blockquote><blockquote name="68bd" id="68bd" class="graf graf--blockquote graf-after--blockquote"><em class="markup--em markup--blockquote-em">AlphaFold 3’s predictions of molecular interactions surpass the accuracy of all existing systems. </em><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">As a single model that computes entire molecular complexes in a holistic way, it’s uniquely able to unify scientific insights.</em></strong></blockquote><blockquote name="3c66" id="3c66" class="graf graf--blockquote graf--empty graf-after--blockquote"><br></blockquote><p name="87a3" id="87a3" class="graf graf--p graf-after--blockquote">While we wait for the final publication, I figured this would be a good time to look into Diffusion Models and how they are pushing the boundaries in a few different domains.</p><p name="e3c7" id="e3c7" class="graf graf--p graf-after--p">The article is split largely into two ‘parts’. The first part goes over the background of Diffusion-</p><ul class="postList"><li name="04ba" id="04ba" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">What are Diffusion Models in Machine Learning</strong>: Diffusion Models are generative models (they generate data similar to what they were trained on).<em class="markup--em markup--li-em"> </em>Diffusion Models follow 2 simple steps. First, we destroy training data by incrementally adding Gaussian noise. Training consists of recovering the data by reversing this noising process. A well-trained Diffusion Model can create whatever we want from a pool of random noise. Replace noise with a space of embeddings, and you can probably see why we’re cooking here.</li></ul><figure name="fe7d" id="fe7d" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*EPvtrpD_kVuSRY9l" data-width="1181" data-height="318" src="https://cdn-images-1.medium.com/max/800/0*EPvtrpD_kVuSRY9l"></figure><ul class="postList"><li name="52b1" id="52b1" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">The advantages of DMs: </strong>Diffusion models have 3 major advantages that make them strong contenders for your generation based tasks- <strong class="markup--strong markup--li-strong">High-Quality Generation:</strong> Diffusion models generate data with exceptional quality and realism, surpassing previous generative models in many tasks; <strong class="markup--strong markup--li-strong">Versatility:</strong> They are applicable to a wide range of data modalities, including images, audio, molecules, etc; and <strong class="markup--strong markup--li-strong">Controllability:</strong> Diffusion models offer a degree of control over the generation process, allowing users to guide the output based on specific requirements or conditions.</li><li name="01da" id="01da" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">The Drawback of Diffusion Models- </strong>As should be evident by their design, DMs are very expensive. There is research to mitigate their costs, but this is still a sore spot for DMs.</li><li name="a622" id="a622" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Why Diffusion Models Work Well (speculation by me)- </strong>By their very nature, Diffusion Models are trained to look at data points holistically at every inference step. Compared to generators like GANs- Diffusion has the luxury of creating outputs in multiple steps, giving us finer control (think about how hard it is to do anything complex in one go). Compared to auto-regression (used by LLMs like ChatGPT)- DMs have greater flexibility. Lastly, the process of noising and denoising plays a similar role to strong data augmentation, wherein the model is forced to build deeper relationships for features. We can also chain Diffusion Models with other models very well, leading to very cool applications</li></ul><figure name="74a9" id="74a9" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*9UFWuJV-0C8BB_I7" data-width="906" data-height="581" src="https://cdn-images-1.medium.com/max/800/0*9UFWuJV-0C8BB_I7"></figure><p name="bba1" id="bba1" class="graf graf--p graf-after--figure">Part 2 is a look at Diffusion Models in multiple fields to show how versatile Diffusion Models can be. These include-</p><ol class="postList"><li name="b01f" id="b01f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Image Generation and Manipulation: </strong>This is what they became famous for. Diffusion models excel in image-related tasks due to their ability to learn the complex distribution of natural images. By gradually denoising random noise, they generate images with remarkable fidelity and diversity, making them ideal for creative applications and image restoration tasks. <strong class="markup--strong markup--li-strong">There is also some very exciting research using Diffusion Models for the reconstruction of Medical Images</strong>. We’ll cover all of this.</li><li name="f58f" id="f58f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Audio Generation and Processing: </strong>The sequential nature of diffusion models makes them well-suited for audio generation and processing. They can capture the temporal dependencies within audio data, leading to realistic and high-quality audio synthesis and enhancement. We can also combine the two do video generation.</li><li name="84fb" id="84fb" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Molecule Design and Drug Discovery: </strong>Diffusion models offer a novel approach to molecule design by navigating the vast chemical space with ease. They can learn the underlying patterns in molecular structures, enabling the generation of molecules with desired properties for pharmaceutical and materials science applications.</li><li name="609d" id="609d" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Language Models: </strong>One of my inspirations for creating this article, Diffusion has shown some promise in NLP and text generation. Text Diffusion might be the next frontier of LLMs, at least for specific types of tasks. We will discuss the minutiae of Autoregressive vs Diffusion based text generation later in this article.</li><li name="385e" id="385e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Temporal Data Modeling:</strong> Diffusion models are adept at handling sequential data like time series. They can fill in missing data points (imputation), predict future values (forecasting), and generate realistic audio waveforms (waveform signal processing).</li><li name="c559" id="c559" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Robust Learning:</strong> Diffusion models contribute to building more robust AI systems. They can “purify” images corrupted by adversarial attacks, removing malicious noise and restoring the original image. This helps make AI models more resilient to manipulation.</li></ol><p name="98e3" id="98e3" class="graf graf--p graf-after--li">The rest of this article will explore these ideas in more detail. As we dig into the ideas (especially part 2)- I feel compelled to stress one point. <em class="markup--em markup--p-em">My writing makes no claims of academic respectability or neutrality.</em> A technique like Diffusion has tons of usage and relevant papers that you could refer to. This article <strong class="markup--strong markup--p-strong">is not</strong> a comprehensive overview of the technique. <em class="markup--em markup--p-em">I talked to a few experts, looked at different publications, picked the ones that I found most interesting/useful, and wrote/experimented based on them. </em>There are lots of use cases/publications that I missed or chose to skip. Do your research to evaluate its utility to your specific use case. <strong class="markup--strong markup--p-strong">This piece (all of my work) is intended to serve as a foundation for your own exploration into the topic rather than the final answer to your questions</strong>.</p><figure name="df9b" id="df9b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*DWjOywoYB6JllJQo.png" data-width="812" data-height="961" src="https://cdn-images-1.medium.com/max/800/0*DWjOywoYB6JllJQo.png"></figure><p name="f0da" id="f0da" class="graf graf--p graf-after--figure">Diffusion is much deeper than a lot of people realize.</p><p name="1e56" id="1e56" class="graf graf--p graf-after--p">7BBV — Enzyme: AlphaFold 3’s prediction for a molecular complex featuring an enzyme protein (blue), an ion (yellow sphere) and simple sugars (yellow), along with the true structure (gray). This enzyme is found in a soil-borne fungus (Verticillium dahliae) that damages a wide range of plants. Insights into how this enzyme interacts with plant cells could help researchers develop healthier, more resilient crops.</p><p name="40c2" id="40c2" class="graf graf--p graf-after--p"><a href="https://artificialintelligencemadesimple.substack.com/" data-href="https://artificialintelligencemadesimple.substack.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">Join 150K+ tech leaders and get insights on the most important ideas in AI straight to your inbox through my free newsletter- AI Made Simple</strong></a></p><h3 name="6bca" id="6bca" class="graf graf--h3 graf-after--p">Part 1: Understanding Diffusion Models</h3><p name="ba74" id="ba74" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">Skip this section if you understand Diffusion Models (or scroll down to the relevant subsections)</em></p><h3 name="c12a" id="c12a" class="graf graf--h3 graf-after--p">What are Diffusion Models</h3><p name="0740" id="0740" class="graf graf--p graf-after--h3">As we talked about, Diffusion Models are based on the noising and denoising input. While the details vary, we can boil down Diffusion-based Generation into two steps-</p><p name="0ea1" id="0ea1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Forward Diffusion: </strong>We take a data sample, like an image, and iteratively add small amounts of Gaussian noise at each step. This slowly corrupts the image until it becomes unrecognizable noise. The model learns the pattern of noise added at each step. This is crucial for the reverse process.</p><figure name="f639" id="f639" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*pIs4AVnVJ7BB3QmA.png" data-width="1400" data-height="227" src="https://cdn-images-1.medium.com/max/800/0*pIs4AVnVJ7BB3QmA.png"></figure><p name="d239" id="d239" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Reverse Diffusion: </strong>We begin with the pure noise from step 1 as input. The model predicts the noise added at each step in the forward process and removes it. This progressively denoises the input, gradually transforming it into a meaningful data sample.</p><figure name="e413" id="e413" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*vqfLAxjuFCWjiyPc.png" data-width="1025" data-height="173" src="https://cdn-images-1.medium.com/max/800/0*vqfLAxjuFCWjiyPc.png"></figure><p name="8232" id="8232" class="graf graf--p graf-after--figure">At the risk of oversimplification, we will move from here. Diffusion has a lot of important math details that are hidden in the details, but I think it’s more important to discuss how/why Diffusion is being used to solve various challenges. I also don’t have any insightful commentary to add to the math/derivation. If you’re interested in that, please refer to either Weng’s aforementioned article or check out this piece by Assembly AI.</p><figure name="ae9a" id="ae9a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*B1HJDHZKIJBKPKPI.png" data-width="1400" data-height="1034" src="https://cdn-images-1.medium.com/max/800/0*B1HJDHZKIJBKPKPI.png"></figure><p name="e36e" id="e36e" class="graf graf--p graf-after--figure">An example of training a diffusion model for modeling a 2D swiss roll data. (Image source: <a href="https://arxiv.org/abs/1503.03585" data-href="https://arxiv.org/abs/1503.03585" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Sohl-Dickstein et al., 2015</a>)</p><p name="d9b0" id="d9b0" class="graf graf--p graf-after--p">For now, we move on to the next section. Why use Diffusion Models when other techniques exist (as you might imagine, they can be very expensive)? There are two ways to answer this question. First, we will talk about why DMs are good. Then, I will speculate why Diffusion is an advantage over techniques.</p><h3 name="81e7" id="81e7" class="graf graf--h3 graf-after--p">Their High Cost</h3><p name="4d3b" id="4d3b" class="graf graf--p graf-after--h3">The iterative nature of the generation process, involving numerous denoising steps, demands substantial computing power and time, especially for high-resolution data. This makes them less practical for real-time applications or resource-constrained environments and has been a huge blocker.</p><figure name="0b4b" id="0b4b" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*pLcSSXU-BoVFixo1.jpeg" data-width="1400" data-height="1050" src="https://cdn-images-1.medium.com/max/800/0*pLcSSXU-BoVFixo1.jpeg"></figure><p name="64ff" id="64ff" class="graf graf--p graf-after--figure">I refuse to use Stock Photos or click pictures of actual cats myself b/c I am cutting-edge</p><p name="b598" id="b598" class="graf graf--p graf-after--p">To enhance the efficiency of DMs, researchers are exploring several avenues. <strong class="markup--strong markup--p-strong">Optimized sampling techniques</strong> aim to reduce the number of denoising steps while maintaining sample quality. This includes employing smarter discretization schemes, developing specialized ODE/SDE solvers tailored for diffusion, and leveraging knowledge distillation to train faster samplers.</p><p name="a2d9" id="a2d9" class="graf graf--p graf-after--p">Furthermore, exploring <strong class="markup--strong markup--p-strong">latent space diffusion</strong> can significantly reduce computational burden by performing the diffusion process in a lower-dimensional representation of the data.</p><figure name="3d6a" id="3d6a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*12KAZy9y5-7xXLAd.png" data-width="1317" data-height="605" src="https://cdn-images-1.medium.com/max/800/0*12KAZy9y5-7xXLAd.png"></figure><p name="e1c1" id="e1c1" class="graf graf--p graf-after--figure"><a href="https://arxiv.org/abs/2112.10752" data-href="https://arxiv.org/abs/2112.10752" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">High-Resolution Image Synthesis with Latent Diffusion Models</a></p><p name="2520" id="2520" class="graf graf--p graf-after--p">Lastly, it’s always good to combine DMs with other techniques like compression and other generators to boost efficiency.</p><h3 name="5205" id="5205" class="graf graf--h3 graf-after--p">What makes Diffusion Models Useful</h3><p name="c985" id="c985" class="graf graf--p graf-after--h3">At its core, the entire Diffusion process gives us three advantages</p><ul class="postList"><li name="d168" id="d168" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">High-Quality Generation:</strong> Diffusion models generate data with exceptional quality and realism, often surpassing previous generative models in many tasks. This stems from their ability to learn the underlying data distribution in a granular manner through the iterative denoising process. The slow and steady refinement from pure noise to a coherent data sample results in highly realistic outputs.</li></ul><figure name="a5b5" id="a5b5" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*evohP-likGMUaEc0.png" data-width="670" data-height="317" src="https://cdn-images-1.medium.com/max/800/0*evohP-likGMUaEc0.png"></figure><p name="ebe2" id="ebe2" class="graf graf--p graf-after--figure"><a href="https://arxiv.org/abs/2112.10741" data-href="https://arxiv.org/abs/2112.10741" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</a></p><ul class="postList"><li name="694a" id="694a" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Versatility:</strong> Diffusion models are remarkably flexible and can be applied to a wide range of data modalities, including images, audio, molecules, and more. This versatility comes from the model’s core mechanism of manipulating noise, a concept that can be applied to any data type represented digitally. Whether it’s pixels in an image, amplitudes in a sound wave, or atoms in a molecule, diffusion models can learn to generate and manipulate them. Diffusion can also be molded into different use-cases.</li><li name="1e18" id="1e18" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Step-by-Step Control:</strong> The step-by-step generation process in diffusion models allows users to exert greater control over the final output. Unlike traditional generative models that produce an output in one shot, diffusion models progressively refine the generated data from noise to a final sample. This gives us greater transparency and the ability to jump in the middle to take experiments in new directions.</li></ul><p name="f810" id="f810" class="graf graf--p graf-after--li">Okay but why do DMs work as well as they do? I couldn’t find any concrete theoretical explanations, so let’s do some theory-crafting.</p><h3 name="b80a" id="b80a" class="graf graf--h3 graf-after--p">Why Diffusion Models are So Good</h3><p name="ffa6" id="ffa6" class="graf graf--p graf-after--h3">Take a complex generation task like writing this article. Traditional generators like GANs generate everything in one shot. As complexity scales up, this becomes exceedingly difficult. Think about how hard it would be paint a detailed scene by throwing paint at a canvas once. This is essentially what a GAN does. If you’ve read JJK (you absolutely should), this is very similar to the whole “giving someone water w/o giving them the bottle” explanation for Sukuna’s Domain.</p><figure name="1b41" id="1b41" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*SWUn8RxflUbyTdwr.jpg" data-width="828" data-height="598" src="https://cdn-images-1.medium.com/max/800/0*SWUn8RxflUbyTdwr.jpg"></figure><p name="e304" id="e304" class="graf graf--p graf-after--figure">Does Gege love Sukuna a bit too much? Discuss-<a href="https://animehunch.com/what-does-sukunas-malevolent-shrine-do-in-jujutsu-kaisen/" data-href="https://animehunch.com/what-does-sukunas-malevolent-shrine-do-in-jujutsu-kaisen/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Img Source</a></p><p name="4e1a" id="4e1a" class="graf graf--p graf-after--p">This is why modern text generators are based on Autoregression. AR scales a lot better, which gives our models the ability to handle more complex tasks. Since AR goes step by step, you also have the ability to stop generation mid-way through or take it in a new direction. Those are two advantages of Autoregression over trad generators. However, AR models can get lost in their sauce (I’m sure we’ve all had experiences with this). Going back to our article example- it’s really hard to write good articles just using autocomplete without having a very clear picture of what you want to do. Pure AR also degrades quickly b/c we can’t go back and edit previously generated components (if I was writing an article word by word, then I could not go back to an earlier para for restructuring).</p><p name="2ec8" id="2ec8" class="graf graf--p graf-after--p">Diffusion has the same step-by-step advantage as AR but with a twist. Since we denoise/input the entire input at every time step, Diffusion allows us to be a lot more contextual. This is closest to how I write. I generally have a fuzzy idea of what I want to cover, which is refined as I write more. At every step, I might go back and fix a much earlier component. This makes the end result much more cohesive.</p><p name="c9ae" id="c9ae" class="graf graf--p graf-after--p">With all that background out of the way, let’s finally start looking at the use of Diffusion Models in various contexts.</p><blockquote name="dd0d" id="dd0d" class="graf graf--blockquote graf-after--p"><em class="markup--em markup--blockquote-em">This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model.</em></blockquote><blockquote name="49d3" id="49d3" class="graf graf--blockquote graf-after--blockquote"><em class="markup--em markup--blockquote-em">- Deep Unsupervised Learning using Nonequilibrium Thermodynamics. </em><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">Fun fact- Physics is what inspired Diffusion Models. </em></strong><em class="markup--em markup--blockquote-em">Don’t just study AI/Tech papers.</em><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em"> </em></strong><em class="markup--em markup--blockquote-em">Boxing your knowledge will only hurt you.</em></blockquote><blockquote name="dcae" id="dcae" class="graf graf--blockquote graf--empty graf-after--blockquote"><br></blockquote><h3 name="873f" id="873f" class="graf graf--h3 graf-after--blockquote">Part 2: Generation with Diffusion Models</h3><h3 name="8aec" id="8aec" class="graf graf--h3 graf-after--h3">Vision Related Tasks</h3><p name="aaf2" id="aaf2" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Note: I will be leaving out a LOT of details in the specific image generation/videos. Diffusion for Vision is a massive field, and I’m going to group them into general families. Look into the deets yourself.</strong></p><blockquote name="4b4e" id="4b4e" class="graf graf--blockquote graf-after--p"><em class="markup--em markup--blockquote-em">SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit significantly outperforms state-of-the-art GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores</em></blockquote><blockquote name="2605" id="2605" class="graf graf--blockquote graf-after--blockquote"><em class="markup--em markup--blockquote-em">-SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations</em></blockquote><blockquote name="6ef3" id="6ef3" class="graf graf--blockquote graf--empty graf-after--blockquote"><br></blockquote><p name="61c1" id="61c1" class="graf graf--p graf-after--blockquote">Diffusion Models have shown amazing raw-generation capabilities-</p><figure name="e70f" id="e70f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*uU9wHvrv6vysWn9J.png" data-width="1285" data-height="536" src="https://cdn-images-1.medium.com/max/800/0*uU9wHvrv6vysWn9J.png"></figure><p name="eade" id="eade" class="graf graf--p graf-after--figure">Compared to previous SOTA models, RPG [318] exhibits a superior ability to express intricate and compositional text prompts within generated images (colored text denotes critical part).</p><p name="3f58" id="3f58" class="graf graf--p graf-after--p">But they can go beyond that to help with tasks like-</p><ul class="postList"><li name="378f" id="378f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Super-Resolution:</strong> Enhancing the resolution of low-resolution images to create higher-resolution versions. Diffusion models like SR3 and CDM progressively refine the image by iteratively denoising, resulting in high-quality upscaling. We shared an example earlier (the pig wearing a hat).</li><li name="0b2c" id="0b2c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Editing:</strong> Diffusion can be used for more than just filling in missing or damaged parts of an image. It can be used to fill in entirely new sections in specific sections.</li></ul><figure name="d31b" id="d31b" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*okkOF0XCPH3Z1AeV.png" data-width="981" data-height="840" src="https://cdn-images-1.medium.com/max/800/0*okkOF0XCPH3Z1AeV.png"></figure><p name="cdc2" id="cdc2" class="graf graf--p graf-after--figure">Text-conditional image inpainting examples from GLIDE. The green region is erased, and the model fills it in conditioned on the given prompt. Our model is able to match the style and lighting of the surrounding context to produce a realistic completion.</p><ul class="postList"><li name="8471" id="8471" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Generative Pre-Training:</strong> Diffusion-based models might be very good for pertaining vision models.</li><li name="bd87" id="bd87" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Point Cloud Completion and Generation:</strong> Point clouds are 3D representations of objects. Diffusion models help in in both generation and completion. “<em class="markup--em markup--li-em">Luo et al. 2021 [173] has taken the approach of treating point clouds as particles in a thermodynamic system, using a heat bath to facilitate diffusion from the original distribution to a noise distribution. Meanwhile, the Point-Voxel Diffusion (PVD) model [346] joins denoising diffusion models with the pointvoxel representation of 3D shapes. The Point Diffusion-Refinement (PDR) model [178] uses a conditional DDPM to generate a coarse completion from partial observations; it also establishes a point-wise mapping between the generated point cloud and the ground truth.</em>”</li><li name="87d3" id="87d3" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Medical Image Reconstruction- </strong>Medical Images are expensive to obtain. They are even harder to annotate since only professionals can do that. (as much as you love me, you probably don’t want me looking at your X-rays to tell you if your bones are in good condition). DMs have shown great promise in reconstructing Medical Images.</li><li name="7b98" id="7b98" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Robust Learning: </strong>Diffusion models can be used to purify adversarial examples by adding noise and reconstructing clean versions, <strong class="markup--strong markup--li-strong">mitigating the impact of adversarial perturbations. </strong>We can also diffusion-based pre-processing steps to enhance the robustness of models against adversarial attacks.</li></ul><figure name="019f" id="019f" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*LSrf-BVD33GPVUYV.png" data-width="1302" data-height="516" src="https://cdn-images-1.medium.com/max/800/0*LSrf-BVD33GPVUYV.png"></figure><p name="0e46" id="0e46" class="graf graf--p graf-after--figure"><a href="https://arxiv.org/abs/2305.16494" data-href="https://arxiv.org/abs/2305.16494" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability</a></p><ul class="postList"><li name="a115" id="a115" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Anomaly Detection: </strong>Diffusion models can be applied to identify unusual or unexpected patterns in images. “<em class="markup--em markup--li-em">These approaches may perform better than alternatives based on adversarial training as they can better model smaller datasets with effective sampling and stable training schemes.</em>”</li></ul><figure name="2f4f" id="2f4f" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*_Yo7RpBOcSnCmc8w.png" data-width="1200" data-height="663" src="https://cdn-images-1.medium.com/max/800/0*_Yo7RpBOcSnCmc8w.png"></figure><p name="c01b" id="c01b" class="graf graf--p graf-after--figure">Chaining techniques together results in very powerful outcomes.</p><h3 name="faf9" id="faf9" class="graf graf--h3 graf-after--p">Text Diffusion for Natural Language Processing and LLMs</h3><p name="0b90" id="0b90" class="graf graf--p graf-after--h3">I’ve been researching a lot of large context-length RAG for some clients, and that’s when I came across, “Transfer Learning for Text Diffusion Models”. This report wants to “<em class="markup--em markup--p-em">see whether pretrained AR models can be transformed into text diffusion models through a lightweight adaptation procedure we call ``AR2Diff’’</em>”.</p><figure name="9092" id="9092" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*34n-H1YvGKNcwlOZ.png" data-width="742" data-height="236" src="https://cdn-images-1.medium.com/max/800/0*34n-H1YvGKNcwlOZ.png"></figure><p name="5d2b" id="5d2b" class="graf graf--p graf-after--figure">Illustration of our AR2Diff method. 1) Pretrain an AR decoder with causal attention. 2) Continue pretraining as a diffusion model with bidirectional attention. 3) Fine-tune as a diffusion model on the end task</p><p name="33b5" id="33b5" class="graf graf--p graf-after--p">While text diffusion lags behind in machine translation, it shows promise in code synthesis and question answering, even outperforming autoregressive models. These findings suggest text diffusion, <strong class="markup--strong markup--p-strong">which can be faster for long text</strong>, deserves further exploration.</p><figure name="4893" id="4893" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*-C77UHkudATFyKQR.png" data-width="808" data-height="575" src="https://cdn-images-1.medium.com/max/800/0*-C77UHkudATFyKQR.png"></figure><p name="158a" id="158a" class="graf graf--p graf-after--figure">Performance of various models across three tasks and three sizes, comparing: (i) an AR baseline, (ii) a diffusion baseline, and (iii) AR2Diff models that adapt the pretrained AR baseline via diffusion training for N steps before fine-tuning using diffusion, with N ∈ {0, 10K, 100K}.</p><p name="a91e" id="a91e" class="graf graf--p graf-after--p">Microsoft’s GENIE, introduced in the paper “Text generation with diffusion language models: a pre-training approach with continuous paragraph denoise” is another interesting example of Diffusion for LLMs.</p><blockquote name="1206" id="1206" class="graf graf--blockquote graf-after--p"><em class="markup--em markup--blockquote-em">GENIE is a large-scale pre-trained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. … </em><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autoregressive models on these benchmarks, and generates more diverse text samples.</em></strong></blockquote><blockquote name="900b" id="900b" class="graf graf--blockquote graf--empty graf-after--blockquote"><br></blockquote><p name="7600" id="7600" class="graf graf--p graf-after--blockquote">I don’t know if it’s just me, but Text Diffusion seems to act as a bridge b/w Encoder-based and Decoder-based LMs. This is why I’m particularly excited about their potential.</p><h3 name="6bf1" id="6bf1" class="graf graf--h3 graf-after--p">Audio + Video Generation</h3><p name="e062" id="e062" class="graf graf--p graf-after--h3">A lot of high-quality audio and video generators also rely on Diffusion Models. DMs have been making waves in Text-Audio Generation: <em class="markup--em markup--p-em">“Grad-TTS [215] presents a novel text-to-speech model with a score-based decoder and diffusion models. It gradually transforms noise predicted by the encoder and is further aligned with text input by the method of Monotonic Alignment Search [219]. Grad-TTS2 [136] improves Grad-TTS in an adaptive way. Diffsound [310] presents a non-autoregressive decoder based on the discrete diffusion model [6, 254], which predicts all the mel-spectrogram tokens in every single step, and then refines the predicted tokens in the following steps. EdiTTS [267] leverages the score-based text-to-speech model to refine a mel-spectrogram prior that is coarsely modified. Instead of estimating the gradient of data density, ProDiff [109] parameterizes the denoising diffusion model by directly predicting the clean data”</em></p><p name="598e" id="598e" class="graf graf--p graf-after--p">DM-based video editors are also popular, with big names like Imagen being very well known.</p><h3 name="f9d9" id="f9d9" class="graf graf--h3 graf-after--p">Temporal Data Modeling</h3><p name="3d65" id="3d65" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Time Series Imputation:</strong> Missing data can be a huge menace for time-series-based data. Given how they’re trained, it’s not too surprising that DMs can handle data imputation for TS. <strong class="markup--strong markup--p-strong">CSDI </strong>Utilizes score-based diffusion models, trained in a self-supervised manner to capture temporal correlations, for effective time series imputation. “<em class="markup--em markup--p-em">Unlike existing score-based approaches, the conditional diffusion model is explicitly trained for imputation and can exploit correlations between observed values. </em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">On healthcare and environmental data, CSDI improves by 40–65% over existing probabilistic imputation methods</em></strong><em class="markup--em markup--p-em"> on popular performance metrics. In addition, </em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">deterministic imputation by CSDI reduces the error by 5–20% compared to the state-of-the-art deterministic imputation methods</em></strong><em class="markup--em markup--p-em">. Furthermore, CSDI can also be applied to time series interpolation and probabilistic forecasting, and is competitive with existing baselines.</em>”</p><figure name="97ff" id="97ff" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*co4FwIHkEL5ubZrv.png" data-width="1213" data-height="450" src="https://cdn-images-1.medium.com/max/800/0*co4FwIHkEL5ubZrv.png"></figure><p name="5652" id="5652" class="graf graf--p graf-after--figure"><a href="https://arxiv.org/abs/2107.03502" data-href="https://arxiv.org/abs/2107.03502" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation</a></p><p name="0666" id="0666" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Time Series Forecasting:</strong> Predicting future values in a time series, important for various prediction tasks. Take <a href="https://arxiv.org/abs/2101.12072" data-href="https://arxiv.org/abs/2101.12072" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">TimeGrad</strong></a><strong class="markup--strong markup--p-strong">,</strong> an autoregressive model that employs diffusion probabilistic models to estimate gradients of the data distribution. The authors show that the approach “<em class="markup--em markup--p-em">is the new state-of-the-art multivariate probabilistic forecasting method on real-world data sets with thousands of correlated dimensions</em>.”</p><figure name="bebc" id="bebc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*xbm_9Fk61hgfXLSu.png" data-width="1328" data-height="608" src="https://cdn-images-1.medium.com/max/800/0*xbm_9Fk61hgfXLSu.png"></figure><p name="cebe" id="cebe" class="graf graf--p graf-after--figure">Test set CRPSsum comparison (lower is better) of models on six real world data sets. Mean and standard error metrics for TimeGrad obtained by re-training and evaluating 10 times.</p><p name="4044" id="4044" class="graf graf--p graf-after--p">As you can see, Diffusion Models are way more than simple image generators. They also have other uses in material design and drug discovery, but I will cover them in special articles dedicated to those topics (we have articles dedicated to AI for drug discovery, chip design, material development, and more in the works). If you want me to push any of them up top, shoot me a message. Otherwise- take it easy, keep working, and I’ll catch y’all soon. Peace.</p><p name="bcec" id="bcec" class="graf graf--p graf-after--p"><a href="https://artificialintelligencemadesimple.substack.com/p/read-this-if-you-want-to-share-ai" data-href="https://artificialintelligencemadesimple.substack.com/p/read-this-if-you-want-to-share-ai" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">If you liked this article and wish to share it, please refer to the following guidelines.</a></p><p name="c5a3" id="c5a3" class="graf graf--p graf-after--p">That is it for this piece. I appreciate your time. As always, if you’re interested in working with me or checking out my other work, my links will be at the end of this email/post. And if you found value in this write-up, I would appreciate you sharing it with more people. <strong class="markup--strong markup--p-strong">It is word-of-mouth referrals like yours that help me grow.</strong></p><figure name="d39f" id="d39f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*jRWhxZIb2YaByUvC.png" data-width="698" data-height="98" src="https://cdn-images-1.medium.com/max/800/0*jRWhxZIb2YaByUvC.png"><figcaption class="imageCaption">Will Diffusion Models Be The Next Frontier of Deep Learning</figcaption></figure><h3 name="dae2" id="dae2" class="graf graf--h3 graf--empty graf-after--figure"><br></h3><h3 name="3d9b" id="3d9b" class="graf graf--h3 graf-after--h3">AlphaFold 3, LLMs and much more are using diffusion to improve performance</h3><h3 name="87b3" id="87b3" class="graf graf--h3 graf--empty graf-after--h3"><br></h3><figure name="e7fd" id="e7fd" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="0*S6LJnZKy-kKy3eGj.jpeg" data-width="88" data-height="88" src="https://cdn-images-1.medium.com/max/800/0*S6LJnZKy-kKy3eGj.jpeg"></figure><figure name="b849" id="b849" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="0*SWYcdNZp1ZiufOfr.png" data-width="48" data-height="48" src="https://cdn-images-1.medium.com/max/800/0*SWYcdNZp1ZiufOfr.png"></figure><p name="772d" id="772d" class="graf graf--p graf-after--figure"><a href="https://machine-learning-made-simple.medium.com/?source=post_page-----7172bea88581--------------------------------" data-href="https://machine-learning-made-simple.medium.com/?source=post_page-----7172bea88581--------------------------------" class="markup--anchor markup--p-anchor" rel="noopener follow noopener" target="_blank">Devansh</a></p><p name="65ba" id="65ba" class="graf graf--p graf-after--p">·</p><p name="d447" id="d447" class="graf graf--p graf-after--p">Follow</p><p name="dd40" id="dd40" class="graf graf--p graf-after--p">Published in</p><p name="417a" id="417a" class="graf graf--p graf-after--p"><a href="https://medium.datadriveninvestor.com/?source=post_page-----7172bea88581--------------------------------" data-href="https://medium.datadriveninvestor.com/?source=post_page-----7172bea88581--------------------------------" class="markup--anchor markup--p-anchor" rel="noopener  ugc nofollow noopener" target="_blank">DataDrivenInvestor</a></p><p name="22c4" id="22c4" class="graf graf--p graf-after--p">·</p><p name="8898" id="8898" class="graf graf--p graf-after--p">16 min read</p><p name="1080" id="1080" class="graf graf--p graf-after--p">·</p><p name="c395" id="c395" class="graf graf--p graf-after--p">4 days ago</p><p name="2043" id="2043" class="graf graf--p graf-after--p">269</p><p name="58ee" id="58ee" class="graf graf--p graf-after--p">1</p><figure name="cbf7" id="cbf7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*D7Uh60vuxMujYO1A" data-width="1400" data-height="1867" src="https://cdn-images-1.medium.com/max/800/0*D7Uh60vuxMujYO1A"></figure><p name="1534" id="1534" class="graf graf--p graf-after--figure">Photo by <a href="https://unsplash.com/@alishahlakhani?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/@alishahlakhani?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Ali Shah Lakhani</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" data-href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Unsplash</a></p><h3 name="2214" id="2214" class="graf graf--h3 graf-after--p">Executive Highlights (tl;dr of the article)</h3><p name="f79d" id="f79d" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">This article will be long and have a lot of references, so I would suggest bookmarking/saving it, and coming back to it periodically.</strong></p><p name="6de0" id="6de0" class="graf graf--p graf-after--p">Google’s AlphaFold 3 is gaining a lot of attention for its potential to revolutionize bio-tech. One of the key innovations that led to its performance gains over previous methods was its utilization of diffusion models</p><blockquote name="7397" id="7397" class="graf graf--blockquote graf-after--p"><em class="markup--em markup--blockquote-em">AlphaFold 3’s capabilities come from its next-generation architecture and training that now covers all of life’s molecules. At the core of the model is an improved version of our Evoformer module — a deep learning architecture that underpinned AlphaFold 2’s incredible performance. </em><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">After processing the inputs, AlphaFold 3 assembles its predictions using a diffusion network, akin to those found in AI image generators. The diffusion process starts with a cloud of atoms, and over many steps converges on its final, most accurate molecular structure.</em></strong></blockquote><blockquote name="5e47" id="5e47" class="graf graf--blockquote graf-after--blockquote"><em class="markup--em markup--blockquote-em">AlphaFold 3’s predictions of molecular interactions surpass the accuracy of all existing systems. </em><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">As a single model that computes entire molecular complexes in a holistic way, it’s uniquely able to unify scientific insights.</em></strong></blockquote><blockquote name="1524" id="1524" class="graf graf--blockquote graf--empty graf-after--blockquote"><br></blockquote><p name="8f04" id="8f04" class="graf graf--p graf-after--blockquote">While we wait for the final publication, I figured this would be a good time to look into Diffusion Models and how they are pushing the boundaries in a few different domains.</p><p name="b5e3" id="b5e3" class="graf graf--p graf-after--p">The article is split largely into two ‘parts’. The first part goes over the background of Diffusion-</p><ul class="postList"><li name="4fc1" id="4fc1" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">What are Diffusion Models in Machine Learning</strong>: Diffusion Models are generative models (they generate data similar to what they were trained on).<em class="markup--em markup--li-em"> </em>Diffusion Models follow 2 simple steps. First, we destroy training data by incrementally adding Gaussian noise. Training consists of recovering the data by reversing this noising process. A well-trained Diffusion Model can create whatever we want from a pool of random noise. Replace noise with a space of embeddings, and you can probably see why we’re cooking here.</li></ul><figure name="bdba" id="bdba" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*-UFUHUY9Y6c6UW0i" data-width="1181" data-height="318" src="https://cdn-images-1.medium.com/max/800/0*-UFUHUY9Y6c6UW0i"></figure><ul class="postList"><li name="418b" id="418b" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">The advantages of DMs: </strong>Diffusion models have 3 major advantages that make them strong contenders for your generation based tasks- <strong class="markup--strong markup--li-strong">High-Quality Generation:</strong> Diffusion models generate data with exceptional quality and realism, surpassing previous generative models in many tasks; <strong class="markup--strong markup--li-strong">Versatility:</strong> They are applicable to a wide range of data modalities, including images, audio, molecules, etc; and <strong class="markup--strong markup--li-strong">Controllability:</strong> Diffusion models offer a degree of control over the generation process, allowing users to guide the output based on specific requirements or conditions.</li><li name="2e98" id="2e98" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">The Drawback of Diffusion Models- </strong>As should be evident by their design, DMs are very expensive. There is research to mitigate their costs, but this is still a sore spot for DMs.</li><li name="2940" id="2940" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Why Diffusion Models Work Well (speculation by me)- </strong>By their very nature, Diffusion Models are trained to look at data points holistically at every inference step. Compared to generators like GANs- Diffusion has the luxury of creating outputs in multiple steps, giving us finer control (think about how hard it is to do anything complex in one go). Compared to auto-regression (used by LLMs like ChatGPT)- DMs have greater flexibility. Lastly, the process of noising and denoising plays a similar role to strong data augmentation, wherein the model is forced to build deeper relationships for features. We can also chain Diffusion Models with other models very well, leading to very cool applications</li></ul><figure name="e79a" id="e79a" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*JDb3wwtHC3qnvCxs" data-width="906" data-height="581" src="https://cdn-images-1.medium.com/max/800/0*JDb3wwtHC3qnvCxs"></figure><p name="63b6" id="63b6" class="graf graf--p graf-after--figure"><a href="https://encord.com/blog/diffusion-models/" data-href="https://encord.com/blog/diffusion-models/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Source</a></p><p name="f41a" id="f41a" class="graf graf--p graf-after--p">Part 2 is a look at Diffusion Models in multiple fields to show how versatile Diffusion Models can be. These include-</p><ol class="postList"><li name="69d5" id="69d5" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Image Generation and Manipulation: </strong>This is what they became famous for. Diffusion models excel in image-related tasks due to their ability to learn the complex distribution of natural images. By gradually denoising random noise, they generate images with remarkable fidelity and diversity, making them ideal for creative applications and image restoration tasks. <strong class="markup--strong markup--li-strong">There is also some very exciting research using Diffusion Models for the reconstruction of Medical Images</strong>. We’ll cover all of this.</li><li name="cde8" id="cde8" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Audio Generation and Processing: </strong>The sequential nature of diffusion models makes them well-suited for audio generation and processing. They can capture the temporal dependencies within audio data, leading to realistic and high-quality audio synthesis and enhancement. We can also combine the two do video generation.</li><li name="e31b" id="e31b" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Molecule Design and Drug Discovery: </strong>Diffusion models offer a novel approach to molecule design by navigating the vast chemical space with ease. They can learn the underlying patterns in molecular structures, enabling the generation of molecules with desired properties for pharmaceutical and materials science applications.</li><li name="e7a3" id="e7a3" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Language Models: </strong>One of my inspirations for creating this article, Diffusion has shown some promise in NLP and text generation. Text Diffusion might be the next frontier of LLMs, at least for specific types of tasks. We will discuss the minutiae of Autoregressive vs Diffusion based text generation later in this article.</li><li name="096e" id="096e" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Temporal Data Modeling:</strong> Diffusion models are adept at handling sequential data like time series. They can fill in missing data points (imputation), predict future values (forecasting), and generate realistic audio waveforms (waveform signal processing).</li><li name="4418" id="4418" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Robust Learning:</strong> Diffusion models contribute to building more robust AI systems. They can “purify” images corrupted by adversarial attacks, removing malicious noise and restoring the original image. This helps make AI models more resilient to manipulation.</li></ol><p name="0dd6" id="0dd6" class="graf graf--p graf-after--li">The rest of this article will explore these ideas in more detail. As we dig into the ideas (especially part 2)- I feel compelled to stress one point. <em class="markup--em markup--p-em">My writing makes no claims of academic respectability or neutrality.</em> A technique like Diffusion has tons of usage and relevant papers that you could refer to. This article <strong class="markup--strong markup--p-strong">is not</strong> a comprehensive overview of the technique. <em class="markup--em markup--p-em">I talked to a few experts, looked at different publications, picked the ones that I found most interesting/useful, and wrote/experimented based on them. </em>There are lots of use cases/publications that I missed or chose to skip. Do your research to evaluate its utility to your specific use case. <strong class="markup--strong markup--p-strong">This piece (all of my work) is intended to serve as a foundation for your own exploration into the topic rather than the final answer to your questions</strong>.</p><figure name="7721" id="7721" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*Cmf7HzvibEUBS9pr.png" data-width="812" data-height="961" src="https://cdn-images-1.medium.com/max/800/0*Cmf7HzvibEUBS9pr.png"></figure><p name="9e9f" id="9e9f" class="graf graf--p graf-after--figure">Diffusion is much deeper than a lot of people realize.</p><p name="d29f" id="d29f" class="graf graf--p graf--empty graf-after--p"><br></p><p name="317c" id="317c" class="graf graf--p graf-after--p">7BBV — Enzyme: AlphaFold 3’s prediction for a molecular complex featuring an enzyme protein (blue), an ion (yellow sphere) and simple sugars (yellow), along with the true structure (gray). This enzyme is found in a soil-borne fungus (Verticillium dahliae) that damages a wide range of plants. Insights into how this enzyme interacts with plant cells could help researchers develop healthier, more resilient crops.</p><p name="4b97" id="4b97" class="graf graf--p graf-after--p"><a href="https://artificialintelligencemadesimple.substack.com/" data-href="https://artificialintelligencemadesimple.substack.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">Join 150K+ tech leaders and get insights on the most important ideas in AI straight to your inbox through my free newsletter- AI Made Simple</strong></a></p><h3 name="3e01" id="3e01" class="graf graf--h3 graf-after--p">Part 1: Understanding Diffusion Models</h3><p name="75dd" id="75dd" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">Skip this section if you understand Diffusion Models (or scroll down to the relevant subsections)</em></p><h3 name="3682" id="3682" class="graf graf--h3 graf-after--p">What are Diffusion Models</h3><p name="80c4" id="80c4" class="graf graf--p graf-after--h3">As we talked about, Diffusion Models are based on the noising and denoising input. While the details vary, we can boil down Diffusion-based Generation into two steps-</p><p name="d429" id="d429" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Forward Diffusion: </strong>We take a data sample, like an image, and iteratively add small amounts of Gaussian noise at each step. This slowly corrupts the image until it becomes unrecognizable noise. The model learns the pattern of noise added at each step. This is crucial for the reverse process.</p><figure name="a06c" id="a06c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*Us0KsCWmIOiCqT1K.png" data-width="1400" data-height="227" src="https://cdn-images-1.medium.com/max/800/0*Us0KsCWmIOiCqT1K.png"></figure><p name="9322" id="9322" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Reverse Diffusion: </strong>We begin with the pure noise from step 1 as input. The model predicts the noise added at each step in the forward process and removes it. This progressively denoises the input, gradually transforming it into a meaningful data sample.</p><figure name="f4b1" id="f4b1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*goIsju2Z_sqmAerQ.png" data-width="1025" data-height="173" src="https://cdn-images-1.medium.com/max/800/0*goIsju2Z_sqmAerQ.png"></figure><p name="ab31" id="ab31" class="graf graf--p graf-after--figure">At the risk of oversimplification, we will move from here. Diffusion has a lot of important math details that are hidden in the details, but I think it’s more important to discuss how/why Diffusion is being used to solve various challenges. I also don’t have any insightful commentary to add to the math/derivation. If you’re interested in that, please refer to either Weng’s aforementioned article or check out this piece by Assembly AI.</p><figure name="2ab6" id="2ab6" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*qyo1GDeIMj5_Iq8i.png" data-width="1400" data-height="1034" src="https://cdn-images-1.medium.com/max/800/0*qyo1GDeIMj5_Iq8i.png"></figure><p name="bce0" id="bce0" class="graf graf--p graf-after--figure">An example of training a diffusion model for modeling a 2D swiss roll data. (Image source: <a href="https://arxiv.org/abs/1503.03585" data-href="https://arxiv.org/abs/1503.03585" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Sohl-Dickstein et al., 2015</a>)</p><p name="fe24" id="fe24" class="graf graf--p graf-after--p">For now, we move on to the next section. Why use Diffusion Models when other techniques exist (as you might imagine, they can be very expensive)? There are two ways to answer this question. First, we will talk about why DMs are good. Then, I will speculate why Diffusion is an advantage over techniques.</p><h3 name="57c8" id="57c8" class="graf graf--h3 graf-after--p">Their High Cost</h3><p name="a0ee" id="a0ee" class="graf graf--p graf-after--h3">The iterative nature of the generation process, involving numerous denoising steps, demands substantial computing power and time, especially for high-resolution data. This makes them less practical for real-time applications or resource-constrained environments and has been a huge blocker.</p><figure name="3bc5" id="3bc5" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*Nchj-DHKyiI2AW8D.jpeg" data-width="1400" data-height="1050" src="https://cdn-images-1.medium.com/max/800/0*Nchj-DHKyiI2AW8D.jpeg"></figure><p name="0d19" id="0d19" class="graf graf--p graf-after--figure">I refuse to use Stock Photos or click pictures of actual cats myself b/c I am cutting-edge</p><p name="e2c0" id="e2c0" class="graf graf--p graf-after--p">To enhance the efficiency of DMs, researchers are exploring several avenues. <strong class="markup--strong markup--p-strong">Optimized sampling techniques</strong> aim to reduce the number of denoising steps while maintaining sample quality. This includes employing smarter discretization schemes, developing specialized ODE/SDE solvers tailored for diffusion, and leveraging knowledge distillation to train faster samplers.</p><p name="a057" id="a057" class="graf graf--p graf-after--p">Furthermore, exploring <strong class="markup--strong markup--p-strong">latent space diffusion</strong> can significantly reduce computational burden by performing the diffusion process in a lower-dimensional representation of the data.</p><figure name="cf91" id="cf91" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*bnt1DyVo-cbqNtX7.png" data-width="1317" data-height="605" src="https://cdn-images-1.medium.com/max/800/0*bnt1DyVo-cbqNtX7.png"></figure><p name="bd02" id="bd02" class="graf graf--p graf-after--figure"><a href="https://arxiv.org/abs/2112.10752" data-href="https://arxiv.org/abs/2112.10752" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">High-Resolution Image Synthesis with Latent Diffusion Models</a></p><p name="8cea" id="8cea" class="graf graf--p graf-after--p">Lastly, it’s always good to combine DMs with other techniques like compression and other generators to boost efficiency.</p><h3 name="8fc1" id="8fc1" class="graf graf--h3 graf-after--p">What makes Diffusion Models Useful</h3><p name="59b4" id="59b4" class="graf graf--p graf-after--h3">At its core, the entire Diffusion process gives us three advantages</p><ul class="postList"><li name="7f54" id="7f54" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">High-Quality Generation:</strong> Diffusion models generate data with exceptional quality and realism, often surpassing previous generative models in many tasks. This stems from their ability to learn the underlying data distribution in a granular manner through the iterative denoising process. The slow and steady refinement from pure noise to a coherent data sample results in highly realistic outputs.</li></ul><figure name="b4cb" id="b4cb" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*WkCKfCHcQ0tKwl_S.png" data-width="670" data-height="317" src="https://cdn-images-1.medium.com/max/800/0*WkCKfCHcQ0tKwl_S.png"></figure><p name="db2b" id="db2b" class="graf graf--p graf-after--figure"><a href="https://arxiv.org/abs/2112.10741" data-href="https://arxiv.org/abs/2112.10741" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</a></p><ul class="postList"><li name="68e0" id="68e0" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Versatility:</strong> Diffusion models are remarkably flexible and can be applied to a wide range of data modalities, including images, audio, molecules, and more. This versatility comes from the model’s core mechanism of manipulating noise, a concept that can be applied to any data type represented digitally. Whether it’s pixels in an image, amplitudes in a sound wave, or atoms in a molecule, diffusion models can learn to generate and manipulate them. Diffusion can also be molded into different use-cases.</li><li name="c907" id="c907" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Step-by-Step Control:</strong> The step-by-step generation process in diffusion models allows users to exert greater control over the final output. Unlike traditional generative models that produce an output in one shot, diffusion models progressively refine the generated data from noise to a final sample. This gives us greater transparency and the ability to jump in the middle to take experiments in new directions.</li></ul><p name="52ce" id="52ce" class="graf graf--p graf-after--li">Okay but why do DMs work as well as they do? I couldn’t find any concrete theoretical explanations, so let’s do some theory-crafting.</p><h3 name="b55f" id="b55f" class="graf graf--h3 graf-after--p">Why Diffusion Models are So Good</h3><p name="08e9" id="08e9" class="graf graf--p graf-after--h3">Take a complex generation task like writing this article. Traditional generators like GANs generate everything in one shot. As complexity scales up, this becomes exceedingly difficult. Think about how hard it would be paint a detailed scene by throwing paint at a canvas once. This is essentially what a GAN does. If you’ve read JJK (you absolutely should), this is very similar to the whole “giving someone water w/o giving them the bottle” explanation for Sukuna’s Domain.</p><figure name="73c3" id="73c3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*S-JGMnOXiBb_WUGd.jpg" data-width="828" data-height="598" src="https://cdn-images-1.medium.com/max/800/0*S-JGMnOXiBb_WUGd.jpg"></figure><p name="91c2" id="91c2" class="graf graf--p graf-after--figure">Does Gege love Sukuna a bit too much? Discuss-<a href="https://animehunch.com/what-does-sukunas-malevolent-shrine-do-in-jujutsu-kaisen/" data-href="https://animehunch.com/what-does-sukunas-malevolent-shrine-do-in-jujutsu-kaisen/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Img Source</a></p><p name="0aca" id="0aca" class="graf graf--p graf-after--p">This is why modern text generators are based on Autoregression. AR scales a lot better, which gives our models the ability to handle more complex tasks. Since AR goes step by step, you also have the ability to stop generation mid-way through or take it in a new direction. Those are two advantages of Autoregression over trad generators. However, AR models can get lost in their sauce (I’m sure we’ve all had experiences with this). Going back to our article example- it’s really hard to write good articles just using autocomplete without having a very clear picture of what you want to do. Pure AR also degrades quickly b/c we can’t go back and edit previously generated components (if I was writing an article word by word, then I could not go back to an earlier para for restructuring).</p><p name="c2e8" id="c2e8" class="graf graf--p graf-after--p">Diffusion has the same step-by-step advantage as AR but with a twist. Since we denoise/input the entire input at every time step, Diffusion allows us to be a lot more contextual. This is closest to how I write. I generally have a fuzzy idea of what I want to cover, which is refined as I write more. At every step, I might go back and fix a much earlier component. This makes the end result much more cohesive.</p><p name="ce45" id="ce45" class="graf graf--p graf-after--p">With all that background out of the way, let’s finally start looking at the use of Diffusion Models in various contexts.</p><blockquote name="8c0f" id="8c0f" class="graf graf--blockquote graf-after--p"><em class="markup--em markup--blockquote-em">This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model.</em></blockquote><blockquote name="743c" id="743c" class="graf graf--blockquote graf-after--blockquote"><em class="markup--em markup--blockquote-em">- Deep Unsupervised Learning using Nonequilibrium Thermodynamics. </em><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">Fun fact- Physics is what inspired Diffusion Models. </em></strong><em class="markup--em markup--blockquote-em">Don’t just study AI/Tech papers.</em><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em"> </em></strong><em class="markup--em markup--blockquote-em">Boxing your knowledge will only hurt you.</em></blockquote><blockquote name="f87c" id="f87c" class="graf graf--blockquote graf--empty graf-after--blockquote"><br></blockquote><h3 name="a54c" id="a54c" class="graf graf--h3 graf-after--blockquote">Part 2: Generation with Diffusion Models</h3><h3 name="6a1e" id="6a1e" class="graf graf--h3 graf-after--h3">Vision Related Tasks</h3><p name="6dfc" id="6dfc" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Note: I will be leaving out a LOT of details in the specific image generation/videos. Diffusion for Vision is a massive field, and I’m going to group them into general families. Look into the deets yourself.</strong></p><blockquote name="c677" id="c677" class="graf graf--blockquote graf-after--p"><em class="markup--em markup--blockquote-em">SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit significantly outperforms state-of-the-art GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores</em></blockquote><blockquote name="9152" id="9152" class="graf graf--blockquote graf-after--blockquote"><em class="markup--em markup--blockquote-em">-SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations</em></blockquote><blockquote name="f367" id="f367" class="graf graf--blockquote graf--empty graf-after--blockquote"><br></blockquote><p name="38f3" id="38f3" class="graf graf--p graf-after--blockquote">Diffusion Models have shown amazing raw-generation capabilities-</p><figure name="c603" id="c603" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*OSr_e8mf-vizF_ah.png" data-width="1285" data-height="536" src="https://cdn-images-1.medium.com/max/800/0*OSr_e8mf-vizF_ah.png"></figure><p name="521d" id="521d" class="graf graf--p graf-after--figure">Compared to previous SOTA models, RPG [318] exhibits a superior ability to express intricate and compositional text prompts within generated images (colored text denotes critical part).</p><p name="c8da" id="c8da" class="graf graf--p graf-after--p">But they can go beyond that to help with tasks like-</p><ul class="postList"><li name="3505" id="3505" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Super-Resolution:</strong> Enhancing the resolution of low-resolution images to create higher-resolution versions. Diffusion models like SR3 and CDM progressively refine the image by iteratively denoising, resulting in high-quality upscaling. We shared an example earlier (the pig wearing a hat).</li><li name="25c4" id="25c4" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Editing:</strong> Diffusion can be used for more than just filling in missing or damaged parts of an image. It can be used to fill in entirely new sections in specific sections.</li></ul><figure name="6660" id="6660" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*5ZFsRymXk-j-b51I.png" data-width="981" data-height="840" src="https://cdn-images-1.medium.com/max/800/0*5ZFsRymXk-j-b51I.png"></figure><p name="6a7b" id="6a7b" class="graf graf--p graf-after--figure">Text-conditional image inpainting examples from GLIDE. The green region is erased, and the model fills it in conditioned on the given prompt. Our model is able to match the style and lighting of the surrounding context to produce a realistic completion.</p><ul class="postList"><li name="88ca" id="88ca" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Generative Pre-Training:</strong> Diffusion-based models might be very good for pertaining vision models.</li><li name="39c6" id="39c6" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Point Cloud Completion and Generation:</strong> Point clouds are 3D representations of objects. Diffusion models help in in both generation and completion. “<em class="markup--em markup--li-em">Luo et al. 2021 [173] has taken the approach of treating point clouds as particles in a thermodynamic system, using a heat bath to facilitate diffusion from the original distribution to a noise distribution. Meanwhile, the Point-Voxel Diffusion (PVD) model [346] joins denoising diffusion models with the pointvoxel representation of 3D shapes. The Point Diffusion-Refinement (PDR) model [178] uses a conditional DDPM to generate a coarse completion from partial observations; it also establishes a point-wise mapping between the generated point cloud and the ground truth.</em>”</li><li name="20f9" id="20f9" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Medical Image Reconstruction- </strong>Medical Images are expensive to obtain. They are even harder to annotate since only professionals can do that. (as much as you love me, you probably don’t want me looking at your X-rays to tell you if your bones are in good condition). DMs have shown great promise in reconstructing Medical Images.</li><li name="b7c2" id="b7c2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Robust Learning: </strong>Diffusion models can be used to purify adversarial examples by adding noise and reconstructing clean versions, <strong class="markup--strong markup--li-strong">mitigating the impact of adversarial perturbations. </strong>We can also diffusion-based pre-processing steps to enhance the robustness of models against adversarial attacks.</li></ul><figure name="743e" id="743e" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*e8-GlO533yLYzO8X.png" data-width="1302" data-height="516" src="https://cdn-images-1.medium.com/max/800/0*e8-GlO533yLYzO8X.png"></figure><p name="d10f" id="d10f" class="graf graf--p graf-after--figure"><a href="https://arxiv.org/abs/2305.16494" data-href="https://arxiv.org/abs/2305.16494" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability</a></p><ul class="postList"><li name="499f" id="499f" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Anomaly Detection: </strong>Diffusion models can be applied to identify unusual or unexpected patterns in images. “<em class="markup--em markup--li-em">These approaches may perform better than alternatives based on adversarial training as they can better model smaller datasets with effective sampling and stable training schemes.</em>”</li></ul><figure name="9938" id="9938" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="0*VGaUvjHfFKQ87Ghx.png" data-width="1200" data-height="663" src="https://cdn-images-1.medium.com/max/800/0*VGaUvjHfFKQ87Ghx.png"></figure><p name="6deb" id="6deb" class="graf graf--p graf-after--figure">Chaining techniques together results in very powerful outcomes.</p><h3 name="c92e" id="c92e" class="graf graf--h3 graf-after--p">Text Diffusion for Natural Language Processing and LLMs</h3><p name="69be" id="69be" class="graf graf--p graf-after--h3">I’ve been researching a lot of large context-length RAG for some clients, and that’s when I came across, “Transfer Learning for Text Diffusion Models”. This report wants to “<em class="markup--em markup--p-em">see whether pretrained AR models can be transformed into text diffusion models through a lightweight adaptation procedure we call ``AR2Diff’’</em>”.</p><figure name="5687" id="5687" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*Zzy3wyGKEbUiL3w-.png" data-width="742" data-height="236" src="https://cdn-images-1.medium.com/max/800/0*Zzy3wyGKEbUiL3w-.png"></figure><p name="7373" id="7373" class="graf graf--p graf-after--figure">Illustration of our AR2Diff method. 1) Pretrain an AR decoder with causal attention. 2) Continue pretraining as a diffusion model with bidirectional attention. 3) Fine-tune as a diffusion model on the end task</p><p name="d88d" id="d88d" class="graf graf--p graf-after--p">While text diffusion lags behind in machine translation, it shows promise in code synthesis and question answering, even outperforming autoregressive models. These findings suggest text diffusion, <strong class="markup--strong markup--p-strong">which can be faster for long text</strong>, deserves further exploration.</p><figure name="9c0d" id="9c0d" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*vAZg4E9qA1OA_-nh.png" data-width="808" data-height="575" src="https://cdn-images-1.medium.com/max/800/0*vAZg4E9qA1OA_-nh.png"></figure><p name="1f7d" id="1f7d" class="graf graf--p graf-after--figure">Performance of various models across three tasks and three sizes, comparing: (i) an AR baseline, (ii) a diffusion baseline, and (iii) AR2Diff models that adapt the pretrained AR baseline via diffusion training for N steps before fine-tuning using diffusion, with N ∈ {0, 10K, 100K}.</p><p name="1193" id="1193" class="graf graf--p graf-after--p">Microsoft’s GENIE, introduced in the paper “Text generation with diffusion language models: a pre-training approach with continuous paragraph denoise” is another interesting example of Diffusion for LLMs.</p><blockquote name="3da8" id="3da8" class="graf graf--blockquote graf-after--p"><em class="markup--em markup--blockquote-em">GENIE is a large-scale pre-trained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. … </em><strong class="markup--strong markup--blockquote-strong"><em class="markup--em markup--blockquote-em">Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autoregressive models on these benchmarks, and generates more diverse text samples.</em></strong></blockquote><blockquote name="650b" id="650b" class="graf graf--blockquote graf--empty graf-after--blockquote"><br></blockquote><p name="d4d0" id="d4d0" class="graf graf--p graf-after--blockquote">I don’t know if it’s just me, but Text Diffusion seems to act as a bridge b/w Encoder-based and Decoder-based LMs. This is why I’m particularly excited about their potential.</p><h3 name="aa9d" id="aa9d" class="graf graf--h3 graf-after--p">Audio + Video Generation</h3><p name="0560" id="0560" class="graf graf--p graf-after--h3">A lot of high-quality audio and video generators also rely on Diffusion Models. DMs have been making waves in Text-Audio Generation: <em class="markup--em markup--p-em">“Grad-TTS [215] presents a novel text-to-speech model with a score-based decoder and diffusion models. It gradually transforms noise predicted by the encoder and is further aligned with text input by the method of Monotonic Alignment Search [219]. Grad-TTS2 [136] improves Grad-TTS in an adaptive way. Diffsound [310] presents a non-autoregressive decoder based on the discrete diffusion model [6, 254], which predicts all the mel-spectrogram tokens in every single step, and then refines the predicted tokens in the following steps. EdiTTS [267] leverages the score-based text-to-speech model to refine a mel-spectrogram prior that is coarsely modified. Instead of estimating the gradient of data density, ProDiff [109] parameterizes the denoising diffusion model by directly predicting the clean data”</em></p><p name="b82d" id="b82d" class="graf graf--p graf-after--p">DM-based video editors are also popular, with big names like Imagen being very well known.</p><h3 name="cdae" id="cdae" class="graf graf--h3 graf-after--p">Temporal Data Modeling</h3><p name="c771" id="c771" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Time Series Imputation:</strong> Missing data can be a huge menace for time-series-based data. Given how they’re trained, it’s not too surprising that DMs can handle data imputation for TS. <strong class="markup--strong markup--p-strong">CSDI </strong>Utilizes score-based diffusion models, trained in a self-supervised manner to capture temporal correlations, for effective time series imputation. “<em class="markup--em markup--p-em">Unlike existing score-based approaches, the conditional diffusion model is explicitly trained for imputation and can exploit correlations between observed values. </em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">On healthcare and environmental data, CSDI improves by 40–65% over existing probabilistic imputation methods</em></strong><em class="markup--em markup--p-em"> on popular performance metrics. In addition, </em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">deterministic imputation by CSDI reduces the error by 5–20% compared to the state-of-the-art deterministic imputation methods</em></strong><em class="markup--em markup--p-em">. Furthermore, CSDI can also be applied to time series interpolation and probabilistic forecasting, and is competitive with existing baselines.</em>”</p><figure name="6cf1" id="6cf1" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*d9khutuGCjRwDIyi.png" data-width="1213" data-height="450" src="https://cdn-images-1.medium.com/max/800/0*d9khutuGCjRwDIyi.png"></figure><p name="1823" id="1823" class="graf graf--p graf-after--figure"><a href="https://arxiv.org/abs/2107.03502" data-href="https://arxiv.org/abs/2107.03502" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation</a></p><p name="cc70" id="cc70" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Time Series Forecasting:</strong> Predicting future values in a time series, important for various prediction tasks. Take <a href="https://arxiv.org/abs/2101.12072" data-href="https://arxiv.org/abs/2101.12072" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">TimeGrad</strong></a><strong class="markup--strong markup--p-strong">,</strong> an autoregressive model that employs diffusion probabilistic models to estimate gradients of the data distribution. The authors show that the approach “<em class="markup--em markup--p-em">is the new state-of-the-art multivariate probabilistic forecasting method on real-world data sets with thousands of correlated dimensions</em>.”</p><figure name="64aa" id="64aa" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*cgyM6hZSvqTH7iuC.png" data-width="1328" data-height="608" src="https://cdn-images-1.medium.com/max/800/0*cgyM6hZSvqTH7iuC.png"></figure><p name="78ac" id="78ac" class="graf graf--p graf-after--figure">Test set CRPSsum comparison (lower is better) of models on six real world data sets. Mean and standard error metrics for TimeGrad obtained by re-training and evaluating 10 times.</p><p name="22d8" id="22d8" class="graf graf--p graf-after--p">As you can see, Diffusion Models are way more than simple image generators. They also have other uses in material design and drug discovery, but I will cover them in special articles dedicated to those topics (we have articles dedicated to AI for drug discovery, chip design, material development, and more in the works). If you want me to push any of them up top, shoot me a message. Otherwise- take it easy, keep working, and I’ll catch y’all soon. Peace.</p><p name="0b1b" id="0b1b" class="graf graf--p graf-after--p"><a href="https://artificialintelligencemadesimple.substack.com/p/read-this-if-you-want-to-share-ai" data-href="https://artificialintelligencemadesimple.substack.com/p/read-this-if-you-want-to-share-ai" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">If you liked this article and wish to share it, please refer to the following guidelines.</a></p><p name="bddb" id="bddb" class="graf graf--p graf-after--p">That is it for this piece. I appreciate your time. As always, if you’re interested in working with me or checking out my other work, my links will be at the end of this email/post. And if you found value in this write-up, I would appreciate you sharing it with more people. <strong class="markup--strong markup--p-strong">It is word-of-mouth referrals like yours that help me grow.</strong></p><figure name="ffb4" id="ffb4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="0*diijFoD3Stf-BVi1.png" data-width="698" data-height="98" src="https://cdn-images-1.medium.com/max/800/0*diijFoD3Stf-BVi1.png"></figure><p name="828d" id="828d" class="graf graf--p graf-after--figure">I put a lot of effort into creating work that is informative, useful, and independent from undue influence. If you’d like to support my writing, please consider becoming a paid subscriber to this newsletter. Doing so helps me put more effort into writing/research, reach more people, and supports my crippling chocolate milk addiction. Help me democratize the most important ideas in AI Research and Engineering to over 100K readers weekly.</p><p name="1f33" id="1f33" class="graf graf--p graf-after--p"><a href="https://artificialintelligencemadesimple.substack.com/subscribe" data-href="https://artificialintelligencemadesimple.substack.com/subscribe" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Help me buy chocolate milk</a></p><p name="150a" id="150a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">PS- We follow a “pay what you can” model, which allows you to support within your means. </em></strong><a href="https://artificialintelligencemadesimple.substack.com/p/help-me-take-ai-made-simple-to-the" data-href="https://artificialintelligencemadesimple.substack.com/p/help-me-take-ai-made-simple-to-the" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Check out this post for more details and to find a plan that works for you</em></strong></a><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">.</em></strong></p><p name="e213" id="e213" class="graf graf--p graf-after--p">I regularly share mini-updates on what I read on the Microblogging sites <a href="https://substack.com/redirect/ff622832-9505-453b-ae09-978237ceceb0?j=eyJ1IjoiNHRuYncifQ.nS0IHgRyrTV_q5g5rpAjxOmBlt9SxaItll-b0wAdi-o" data-href="https://substack.com/redirect/ff622832-9505-453b-ae09-978237ceceb0?j=eyJ1IjoiNHRuYncifQ.nS0IHgRyrTV_q5g5rpAjxOmBlt9SxaItll-b0wAdi-o" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">X(https://twitter.com/Machine01776819</a>), <a href="https://www.threads.net/@iseethings404" data-href="https://www.threads.net/@iseethings404" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Threads(https://www.threads.net/@iseethings404</a>), and <a href="https://www.tiktok.com/@chocolatemilkcultleader?_t=8lBL3GzTyXK&amp;_r=1" data-href="https://www.tiktok.com/@chocolatemilkcultleader?_t=8lBL3GzTyXK&amp;_r=1" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">TikTok(https://www.tiktok.com/@devansh_ai_made_simple</a>)- so follow me there if you’re interested in keeping up with my learnings.</p><h3 name="7290" id="7290" class="graf graf--h3 graf-after--p">Reach out to me</h3><p name="1d2e" id="1d2e" class="graf graf--p graf-after--h3">Use the links below to check out my other content, learn more about tutoring, reach out to me about projects, or just to say hi.</p><p name="6126" id="6126" class="graf graf--p graf-after--p"><a href="https://www.instagram.com/yourgodandsavior/" data-href="https://www.instagram.com/yourgodandsavior/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Small Snippets about Tech, AI and Machine Learning over here</a></p><p name="14c1" id="14c1" class="graf graf--p graf-after--p"><a href="https://artificialintelligencemadesimple.substack.com/" data-href="https://artificialintelligencemadesimple.substack.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">AI Newsletter- https://artificialintelligencemadesimple.substack.com/</a></p><p name="4fb7" id="4fb7" class="graf graf--p graf-after--p"><a href="https://codinginterviewsmadesimple.substack.com/" data-href="https://codinginterviewsmadesimple.substack.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">My grandma’s favorite Tech Newsletter- https://codinginterviewsmadesimple.substack.com/</a></p><p name="8035" id="8035" class="graf graf--p graf-after--p">Check out my other articles on Medium. : <a href="https://machine-learning-made-simple.medium.com/" data-href="https://machine-learning-made-simple.medium.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://rb.gy/zn1aiu</a></p><p name="f49d" id="f49d" class="graf graf--p graf-after--p">My YouTube: <a href="https://rb.gy/88iwdd" data-href="https://rb.gy/88iwdd" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">https://rb.gy/88iwdd</a></p><p name="1e86" id="1e86" class="graf graf--p graf-after--p">Reach out to me on LinkedIn. Let’s connect: <a href="https://www.linkedin.com/in/devansh-devansh-516004168/" data-href="https://www.linkedin.com/in/devansh-devansh-516004168/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">https://rb.gy/m5ok2y</a></p><p name="653e" id="653e" class="graf graf--p graf-after--p">My Instagram: <a href="https://rb.gy/gmvuy9" data-href="https://rb.gy/gmvuy9" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">https://rb.gy/gmvuy9</a></p><p name="1170" id="1170" class="graf graf--p graf-after--p">My Twitter: <a href="https://twitter.com/Machine01776819" data-href="https://twitter.com/Machine01776819" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">https://twitter.com/Machine01776819</a></p><p name="0293" id="0293" class="graf graf--p graf-after--p">I put a lot of effort into creating work that is informative, useful, and independent from undue influence. If you’d like to support my writing, please consider becoming a paid subscriber to this newsletter. Doing so helps me put more effort into writing/research, reach more people, and supports my crippling chocolate milk addiction. Help me democratize the most important ideas in AI Research and Engineering to over 100K readers weekly.</p><p name="56fc" id="56fc" class="graf graf--p graf-after--p"><a href="https://artificialintelligencemadesimple.substack.com/subscribe" data-href="https://artificialintelligencemadesimple.substack.com/subscribe" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Help me buy chocolate milk</a></p><p name="5c9f" id="5c9f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">PS- We follow a “pay what you can” model, which allows you to support within your means. </em></strong><a href="https://artificialintelligencemadesimple.substack.com/p/help-me-take-ai-made-simple-to-the" data-href="https://artificialintelligencemadesimple.substack.com/p/help-me-take-ai-made-simple-to-the" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Check out this post for more details and to find a plan that works for you</em></strong></a><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">.</em></strong></p><p name="1314" id="1314" class="graf graf--p graf-after--p">I regularly share mini-updates on what I read on the Microblogging sites <a href="https://substack.com/redirect/ff622832-9505-453b-ae09-978237ceceb0?j=eyJ1IjoiNHRuYncifQ.nS0IHgRyrTV_q5g5rpAjxOmBlt9SxaItll-b0wAdi-o" data-href="https://substack.com/redirect/ff622832-9505-453b-ae09-978237ceceb0?j=eyJ1IjoiNHRuYncifQ.nS0IHgRyrTV_q5g5rpAjxOmBlt9SxaItll-b0wAdi-o" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">X(https://twitter.com/Machine01776819</a>), <a href="https://www.threads.net/@iseethings404" data-href="https://www.threads.net/@iseethings404" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Threads(https://www.threads.net/@iseethings404</a>), and <a href="https://www.tiktok.com/@chocolatemilkcultleader?_t=8lBL3GzTyXK&amp;_r=1" data-href="https://www.tiktok.com/@chocolatemilkcultleader?_t=8lBL3GzTyXK&amp;_r=1" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">TikTok(https://www.tiktok.com/@devansh_ai_made_simple</a>)- so follow me there if you’re interested in keeping up with my learnings.</p><h3 name="a64c" id="a64c" class="graf graf--h3 graf-after--p">Reach out to me</h3><p name="aaf4" id="aaf4" class="graf graf--p graf-after--h3">Use the links below to check out my other content, learn more about tutoring, reach out to me about projects, or just to say hi.</p><p name="5d72" id="5d72" class="graf graf--p graf-after--p"><a href="https://www.instagram.com/yourgodandsavior/" data-href="https://www.instagram.com/yourgodandsavior/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">Small Snippets about Tech, AI and Machine Learning over here</a></p><p name="3d43" id="3d43" class="graf graf--p graf-after--p"><a href="https://artificialintelligencemadesimple.substack.com/" data-href="https://artificialintelligencemadesimple.substack.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">AI Newsletter- https://artificialintelligencemadesimple.substack.com/</a></p><p name="82df" id="82df" class="graf graf--p graf-after--p"><a href="https://codinginterviewsmadesimple.substack.com/" data-href="https://codinginterviewsmadesimple.substack.com/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">My grandma’s favorite Tech Newsletter- https://codinginterviewsmadesimple.substack.com/</a></p><p name="b742" id="b742" class="graf graf--p graf-after--p">Check out my other articles on Medium. : <a href="https://machine-learning-made-simple.medium.com/" data-href="https://machine-learning-made-simple.medium.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://rb.gy/zn1aiu</a></p><p name="1bf4" id="1bf4" class="graf graf--p graf-after--p">My YouTube: <a href="https://rb.gy/88iwdd" data-href="https://rb.gy/88iwdd" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">https://rb.gy/88iwdd</a></p><p name="421b" id="421b" class="graf graf--p graf-after--p">Reach out to me on LinkedIn. Let’s connect: <a href="https://www.linkedin.com/in/devansh-devansh-516004168/" data-href="https://www.linkedin.com/in/devansh-devansh-516004168/" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">https://rb.gy/m5ok2y</a></p><p name="da47" id="da47" class="graf graf--p graf-after--p">My Instagram: <a href="https://rb.gy/gmvuy9" data-href="https://rb.gy/gmvuy9" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">https://rb.gy/gmvuy9</a></p><p name="b13f" id="b13f" class="graf graf--p graf-after--p">My Twitter: <a href="https://twitter.com/Machine01776819" data-href="https://twitter.com/Machine01776819" class="markup--anchor markup--p-anchor" rel="noopener ugc nofollow noopener" target="_blank">https://twitter.com/Machine01776819</a></p><p name="55b0" id="55b0" class="graf graf--p graf--empty graf-after--p graf--trailing"><br></p></div></div></section>
</section>
<footer><p><a href="https://medium.com/p/5cbb026bf728">View original.</a></p><p>Exported from <a href="https://medium.com">Medium</a> on August 6, 2024.</p></footer></article></body></html>